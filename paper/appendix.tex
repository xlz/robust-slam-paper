%\newpage
\appendix
\subsection{SLAM as a Least Squares Problem}
\label{appendix:leastsquare}
For completeness of the paper, we review how to form the SLAM optimization problem as a least squares problem, following the derivation in \cite{isam}. Recall that in equation \ref{eq:MAP}, we need to solve a quadratic program, however we may approximate the terms with first order polynomials. For the process term,
\begin{equation}
\begin{aligned}
f_i(\boldsymbol{x}_{i-1}, \boldsymbol{u}_i) - \boldsymbol{x}_i\\
\approx & {f_i(\boldsymbol{x}_{i-1}^0, \boldsymbol{u}_i)} + \boldsymbol{F}_i^{i-1}\delta \boldsymbol{x}_{i-1} - (\boldsymbol{x}_i^0 + \delta \boldsymbol{x}_i)\\
=& \boldsymbol{F}_i^{i-1}\delta \boldsymbol{x}_{i-1} + \boldsymbol{G}_i^i\delta \boldsymbol{x}_i - \boldsymbol{a}_i \\
\end{aligned}
\label{eq:linearProcessTerm}
\end{equation}
where $\boldsymbol{I}$ is the identity and %resolved\cj{does $I$ refer to the matrix identity? define xlz: yes}
\begin{equation*}
\begin{aligned}
\boldsymbol{F}_i^{i-1} &= \frac{\partial f_i(\boldsymbol{x}_{i-1}, \boldsymbol{u}_i)}{\partial \boldsymbol{x}_{i-1}}|_{\boldsymbol{x}_{i-1}^0}, \label{F} \\%\quad
 \boldsymbol{G}_i^i &= -\boldsymbol{I}, \text{(Identity matrix)}\\% \quad 
\boldsymbol{a}_i &= \boldsymbol{x}_i^0 - f_i(\boldsymbol{x}_{i-1}^0, \boldsymbol{u}_i).
\end{aligned}
\end{equation*}
For the measurement term:
\begin{equation}
\begin{aligned}
h_k(\boldsymbol{x}_{i_k}, \boldsymbol{l}_{j_k}) - \boldsymbol{z}_k\\
\approx & {h_k(\boldsymbol{x}_{i_k}^0, \boldsymbol{l}_{j_k}^0) + \boldsymbol{H}_k^{i_k}\delta \boldsymbol{x}_{i_k} + \boldsymbol{J}_k^{j_k}\delta \boldsymbol{l}_{j_k}} - \boldsymbol{z}_k\\
= & {\boldsymbol{H}_k^{i_k}\delta \boldsymbol{x}_{i_k} + \boldsymbol{J}_k^{j_k}\delta \boldsymbol{l}_{j_k}} - \boldsymbol{c}_k\\
\end{aligned}
\label{eq:linearMeasurementTerm}
\end{equation}
where, %resolved\cj{What is the difference between $\boldsymbol{H}_k^{i_k}$ and $\boldsymbol{J}_k^{j_k}$? Should $\boldsymbol{J}_k^{j_k}$ be with respect to $\boldsymbol{x}_{j_k}$? xlz: fixed, the denominator of $J$ should be $\boldsymbol{l}_{j_k}$}
\begin{equation*}
\begin{aligned}
\boldsymbol{H}_k^{i_k} &= \frac{\partial h_k(\boldsymbol{x}_{i_k}, \boldsymbol{l}_{j_k})}{\partial \boldsymbol{x}_{i_k}}|_{(\boldsymbol{x}_{i_k}^0, \boldsymbol{l}_{j_k}^0)}, \\ 
\boldsymbol{J}_k^{j_k} &= \frac{\partial h_k(\boldsymbol{x}_{i_k}, \boldsymbol{l}_{j_k})}{\partial \boldsymbol{l}_{j_k}}|_{(\boldsymbol{x}_{i_k}^0, \boldsymbol{l}_{j_k}^0)} , \\
 \boldsymbol{c}_k &= \boldsymbol{z}_k - h_k(\boldsymbol{x}_{i_k}^0, \boldsymbol{l}_{j_k}^0)
\end{aligned}
%\label{eq:linearMeasurementTerm}
\end{equation*}
%In our case, 
%\begin{equation}
%\begin{aligned}
%h(\boldsymbol{x}_i, \boldsymbol{l}_j) = 
%\begin{pmatrix}
%\sqrt{(\boldsymbol{x}_i^x - \boldsymbol{l}_j^x)^2 + (\boldsymbol{x}_i^y - \boldsymbol{l}_j^y)^2}\\
%\tan^{-1}(\frac{\boldsymbol{l}_j^y - \boldsymbol{x}_j^y}{\boldsymbol{l}_j^x - \boldsymbol{x}_i^x}) - \thet\boldsymbol{a}_i
%\end{pmatrix}\\
%\end{aligned}
%\label{eq:measurementTermDerivative}
%\end{equation}


Therefore, the optimization problem thus becomes
\begin{equation}
\begin{aligned}
\delta \boldsymbol{\theta}^* = 
\operatorname*{arg\,min}&_{\delta \boldsymbol{\theta}}\{\sum_{i=1}^M||\boldsymbol{F}_i^{i-1}\delta \boldsymbol{x}_{i-1} 
+ \boldsymbol{G}_i^i\delta \boldsymbol{x}_i - \boldsymbol{a}_i||_{\boldsymbol{\Gamma}_i}^2 \\ 
&\hspace{-0.5cm}+ \sum_{k=1}^Kw_{j_k}||v_k(\boldsymbol{H}_k^{i_k}\delta \boldsymbol{x}_{i_k} 
 + \boldsymbol{J}_k^{j_k}\delta \boldsymbol{l}_{j_k} - \boldsymbol{c}_k)||_{\boldsymbol{\Sigma}_k}^2\}
\end{aligned}
\end{equation}

To rewrite the Mahalanobis norm, notice that
\begin{equation}
||\boldsymbol{e}||_{\boldsymbol{\Gamma}}^2 = \boldsymbol{e}^T\boldsymbol{\Gamma}^{-1}\boldsymbol{e} = ||\boldsymbol{\Gamma}^{-T/2}\boldsymbol{e}||^2
\end{equation}

Therefore, we can collect all the Jacobian matrices multiplied by either $\boldsymbol{\Gamma}_i^{-T/2}$ or $v_k w_{j_k}^{1/2}\boldsymbol{\Sigma}_k^{-T/2}$, specifically,

\begin{equation}
\begin{aligned}
\delta \boldsymbol{\theta}^* =& \operatorname*{arg\,min}_{\delta \boldsymbol{\theta}}\{&
\\ &\sum_{i=1}^M||\boldsymbol{\Gamma}_i^{-T/2}\boldsymbol{F}_i^{i-1}\delta \boldsymbol{x}_{i-1} + \boldsymbol{\Gamma}_i^{-T/2} \boldsymbol{G}_i^i\delta \boldsymbol{x}_i - \boldsymbol{\Gamma}_i^{-T/2}\boldsymbol{a}_i||^2  \\
&+\sum_{k=1}^K||v_k\sqrt{w_{j_k}}\boldsymbol{\Sigma}_k^{-T/2} \boldsymbol{H}_k^{i_k}\delta \boldsymbol{x}_{i_k} 
\\ &\hspace{0.5cm}+ v_k\sqrt{w_{j_k}}\boldsymbol{\Sigma}_k^{-T/2}\boldsymbol{J}_k^{j_k}\delta \boldsymbol{l}_{j_k} 
- v_k\sqrt{w_{j_k}}\boldsymbol{\Sigma}_k^{-T/2}\boldsymbol{c}_k||^2\}
\end{aligned}
\end{equation}

We concatenate the matrices $\boldsymbol{F}, \boldsymbol{G}, \boldsymbol{H}, \boldsymbol{J}$ in block form into a square matrix $\boldsymbol{A}$, $\delta \boldsymbol{x}, \delta \boldsymbol{l}$ into $\delta \boldsymbol{\theta}$, and $\boldsymbol{a}$ and $\boldsymbol{c}$ into the residual vector $\boldsymbol{b}$ to then obtain the standard least squares problem:
\begin{equation}
\delta \boldsymbol{\theta}^* = \operatorname*{arg\,min}_{\delta \boldsymbol{\theta}}||\boldsymbol{A}\delta \boldsymbol{\theta} - \boldsymbol{b}||^2
\end{equation}

\section{Augmented Graph SLAM}
\label{sec:model}
%\begin{wrapfigure}{r}{0.25\textwidth}
\begin{figure}[!t]
\begin{center}
%\framebox[4.0in]{$\;$}
%\fbox{\rule[-.5cm]{0cm}{4cm} \rule[-.5cm]{4cm}{0cm}}
 \includegraphics[width=0.5\columnwidth]{fig/model} 
\end{center}
\caption{Graphical Model Formulation. Dark nodes are observed.}
\label{fig:model}
\end{figure}
%\end{wrapfigure}

Following \cite{isam} we formulate the SLAM problem in graphical models as in Figure \ref{fig:model}. Specifically, the robot states are denoted by $X = \{x_i\}$ with $i \in 0, \dots T$, the landmarks by $L = \{l_j\}$ with $j \in 1,\dots, N$, the control inputs by $U = \{u_i\}$ for $i \in 1,\dots, T$ and the landmark measurements by $Z = \{z_k\}$ with $k \in 1, \dots, K$. In addition to the classical graph SLAM formulation, we augment the representation of landmarks with latent parameters $W = \{w_j\}$ with $j \in 1, \dots, N$ \cj{is W for data associateion? if so, this should be stated explicitly}. Then the joint probability of all variables and measurements are given by

%\begin{equation}
\begin{multline}
P(X, L, U, Z, W) \propto  \\
\prod\limits_{i}P(x_i|x_{i-1}, u_i)\prod\limits_{k}P(z_k|x_{i_k}, l_{j_k}, w_{j_k}).
\label{eq:jointProb}
\end{multline}
%\end{equation}

Then the maximum likelihood (ML) estimate of the unobserved poses $X$ and landmarks $L$ given observations $Z$, known controls $U$, and the current latent parameters $W$ is

\begin{equation}
X^*, L^* = \operatorname*{arg\,max}_{X,L} P(X,L,U,Z,W).
\end{equation}

To calculate the ML estimate, the objective is linearized and converted into a linear least
squares problem in this form $\operatorname*{arg\,min}_{\delta} || A
\delta  - b ||^2$ by algebraic manipulation, and then optimized using
different numerical methods. Derivation with more details is provided in appendix \ref{appendix:leastsquare}.

Using a Gaussian representation with the latent extension, the sensor
model, the process model and measurement equation follows

\begin{equation}
\begin{aligned}
x_i &= f_i(x_{i-1}, u_i) + \omega_i \\
z_k &= h_k(x_{i_k}, l_{j_k}) + \nu_k
\end{aligned}
\label{eq:gaussRepresentation}
\end{equation}

where $\omega_i$ and $\nu_k$ are noise terms which follow zero-mean normal distribution with covariance matrices $\Gamma_i$ and $\Sigma_k$. With this formulation, the second part of the joint probability \ref{eq:jointProb} is augmented with the mobility indicator. In addition we also apply a robust kernel to the observation term, then the formulation as a whole is given as:

\cj{need to reformat exp() to be within margin, e is placeholder}
%\begin{equation}
\begin{multline}
P(z_k|x_{i_k}, l_{j_k}, w_{j_k})\propto \\
 e^{(-w_{j_k} (v_k(z_k - h_k(x_{i_k}, l_{j_k})))^T \Sigma_k^{-1} (v_k(z_k - h_k(x_{i_k}, l_{j_k}))) }.
 %\exp(-w_{j_k} (v_k(z_k - h_k(x_{i_k}, l_{j_k})))^T \Sigma_k^{-1} (v_k(z_k - h_k(x_{i_k}, l_{j_k}))) ).
\label{eq:sensor}
\end{multline}
%\end{equation}

where $w_{j_k}$ is associated with each landmark representing the likelihood that the measurement comes from static landmark, and $v_k$ is the robust scaling factor associated with each landmark observation representing whether the measurement is an inlier. When $w_{j_k}$ or $v_k$ approaches zero, the effect is equivalent to making the covariance of the Gaussian very large, effectively rendering the distribution uniform and the constraint represented by the distribution of no impact on the graph optimization process. Note that $w_{j_k}$ can be negative because the formulation given here is proportional to a normalizing constant.
% In our case, we need to infer what their values are and ideally whether the process could be online or folows an incremental fashion. Possible solutions could be 1) using visual cues to cluster the landmarks to ``moving''/``static''. Challenges mainly lies in whether the features we use is reliable or not in terms of this clustering task; 2) instead of using an expensive EM algorithm in \cite{rogers2010slam}, we design particle filters to incrementally update parameter $w_k$, making the whole process online.
